{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Assignment : 14","metadata":{}},{"cell_type":"markdown","source":"<pre>\n1. You can work with preprocessed_data.csv for the assignment. You can get the data from - <a href='https://drive.google.com/drive/u/0/folders/1CJnItndeSSJu7aragQoXWZS9-0apN6pp'>Data folder </a>\n2. Load the data in your notebook.\n3. After step 2 you have to train 3 types of models as discussed below. \n4. For all the model use <a href='https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics'>'auc'</a> as a metric. check <a  href='https://stackoverflow.com/a/46844409'>this</a> and <a  href='https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/80807'>this</a> for using auc as a metric \n5. You are free to choose any number of layers/hiddden units but you have to use same type of architectures shown below. \n6. You can use any one of the optimizers and choice of Learning rate and momentum.\n7. For all the model's use <a href='https://www.youtube.com/watch?v=2U6Jl7oqRkM'>TensorBoard</a> and plot the Metric value and Loss with epoch. While submitting, take a screenshot of plots and include those images in a separate pad and write your observations about them.\n8. Make sure that you are using GPU to train the given models.\n</pre>","metadata":{}},{"cell_type":"code","source":"#you can use gdown modules to import dataset for the assignment\n#for importing any file from drive to Colab you can write the syntax as !gdown --id file_id\n#you can run the below cell to import the required preprocessed data.csv file and glove vector","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!gdown --id 1GpATd_pM4mcnWWIs28-s1lgqdAg2Wdv-\n#!gdown --id 1pGd5tLwA30M7wkbJKdXHaae9tYVDICJ_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom keras import Model,Input\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import LSTM, SpatialDropout1D, BatchNormalization,concatenate,Flatten,Embedding,Dense,Dropout,MaxPooling2D,Reshape, Conv1D\nfrom keras.layers.convolutional import Conv2D,Conv1D\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint,LearningRateScheduler\nfrom keras.initializers import he_normal\nfrom tensorflow.python.keras.callbacks import TensorBoard\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nfrom keras.preprocessing.sequence import pad_sequences\nimport pickle\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:16:50.627050Z","iopub.execute_input":"2022-07-09T19:16:50.627695Z","iopub.status.idle":"2022-07-09T19:16:50.635469Z","shell.execute_reply.started":"2022-07-09T19:16:50.627655Z","shell.execute_reply":"2022-07-09T19:16:50.634554Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## <font color='red'> Model-1 </font>\nBuild and Train deep neural network as shown below","metadata":{}},{"cell_type":"markdown","source":"<img src='https://i.imgur.com/w395Yk9.png'>\nref: https://i.imgur.com/w395Yk9.png","metadata":{}},{"cell_type":"markdown","source":"- __Input_seq_total_text_data__ --- You have to give Total text data columns. After this use the Embedding layer to get word vectors. Use given predefined glove word vectors, don't train any word vectors. After this use LSTM and get the LSTM output and Flatten that output. \n- __Input_school_state__ --- Give 'school_state' column as input to embedding layer and Train the Keras Embedding layer. \n- __Project_grade_category__  --- Give 'project_grade_category' column as input to embedding layer and Train the Keras Embedding layer.\n- __Input_clean_categories__ --- Give 'input_clean_categories' column as input to embedding layer and Train the Keras Embedding layer.\n- __Input_clean_subcategories__ --- Give 'input_clean_subcategories' column as input to embedding layer and Train the Keras Embedding layer.\n- __Input_clean_subcategories__ --- Give 'input_teacher_prefix' column as input to embedding layer and Train the Keras Embedding layer.\n- __Input_remaining_teacher_number_of_previously_posted_projects._resource_summary_contains_numerical_digits._price._quantity__ ---concatenate remaining columns and add a Dense layer after that. \n\n","metadata":{}},{"cell_type":"markdown","source":"Below is an example of embedding layer for a categorical columns. In below code all are dummy values, we gave only for referance. ","metadata":{}},{"cell_type":"code","source":"# # https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n# input_layer = Input(shape=(n,))\n# embedding = Embedding(no_1, no_2, input_length=n)(input_layer)\n# flatten = Flatten()(embedding)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Go through this blog, if you have any doubt on using predefined Embedding values in Embedding layer - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n### 2. Please go through this link https://keras.io/getting-started/functional-api-guide/ and check the 'Multi-input and multi-output models' then you will get to know how to give multiple inputs. ","metadata":{}},{"cell_type":"markdown","source":"# <font color='red'> Model-1 </font>","metadata":{}},{"cell_type":"code","source":"os.chdir(r'D:\\Applied_AI\\Ass_26_LSTM')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import all the libraries\n#make sure that you import your libraries from tf.keras and not just keras\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input,Dense,LSTM","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#read the csv file\nimport pandas as pd\n# df = pd.read_csv('preprocessed_data.csv')\ndf = pd.read_csv('../input/preprocesseddonorchoose/preprocessed_data.csv')\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:17:30.133089Z","iopub.execute_input":"2022-07-09T19:17:30.133437Z","iopub.status.idle":"2022-07-09T19:17:32.399066Z","shell.execute_reply.started":"2022-07-09T19:17:30.133408Z","shell.execute_reply":"2022-07-09T19:17:32.398186Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"X = df.drop(['project_is_approved'], axis=1)\ny = df['project_is_approved']\nX.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:17:37.208506Z","iopub.execute_input":"2022-07-09T19:17:37.209129Z","iopub.status.idle":"2022-07-09T19:17:37.238847Z","shell.execute_reply.started":"2022-07-09T19:17:37.209098Z","shell.execute_reply":"2022-07-09T19:17:37.237549Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# perform stratified train test split on the dataset\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, stratify=y)\nX_tr.shape, X_te.shape, y_tr.shape, y_te.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:17:37.693693Z","iopub.execute_input":"2022-07-09T19:17:37.694265Z","iopub.status.idle":"2022-07-09T19:17:37.797612Z","shell.execute_reply.started":"2022-07-09T19:17:37.694220Z","shell.execute_reply":"2022-07-09T19:17:37.796641Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## 1.1 Text Vectorization","metadata":{}},{"cell_type":"code","source":"#since the data is already preprocessed, we can directly move to vectorization part\n#first we will vectorize the text data\n#for vectorization of text data in deep learning we use tokenizer, you can go through below references\n# https://www.kdnuggets.com/2020/03/tensorflow-keras-tokenization-text-data-prep.html\n#https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do\n# after text vectorization you should get train_padded_docs and test_padded_docs","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:17:38.619579Z","iopub.execute_input":"2022-07-09T19:17:38.620027Z","iopub.status.idle":"2022-07-09T19:17:38.625375Z","shell.execute_reply.started":"2022-07-09T19:17:38.619989Z","shell.execute_reply":"2022-07-09T19:17:38.624315Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"num_words = 1000\noov_token = '<UNK>'\npad_type = 'post'\ntrunc_type = 'post'","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:17:39.041989Z","iopub.execute_input":"2022-07-09T19:17:39.042438Z","iopub.status.idle":"2022-07-09T19:17:39.051729Z","shell.execute_reply.started":"2022-07-09T19:17:39.042399Z","shell.execute_reply":"2022-07-09T19:17:39.050678Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_data = X_tr['essay']\n# Tokenize our training data\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data)\n\n# Get our training data word index\nword_index = tokenizer.word_index\n\n# Encode training data sentences into sequences\ntrain_sequences = tokenizer.texts_to_sequences(train_data)\n\n# Get max training sequence length\nmaxlen = 300\n# Pad the training sequences\ntrain_padded = pad_sequences(train_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\npad_len = train_padded.shape[1]","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:17:39.916972Z","iopub.execute_input":"2022-07-09T19:17:39.917308Z","iopub.status.idle":"2022-07-09T19:17:56.931902Z","shell.execute_reply.started":"2022-07-09T19:17:39.917280Z","shell.execute_reply":"2022-07-09T19:17:56.930894Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"test_data = X_te['essay']\ntest_sequences = tokenizer.texts_to_sequences(test_data)\ntest_padded = pad_sequences(test_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:18:14.169272Z","iopub.execute_input":"2022-07-09T19:18:14.169674Z","iopub.status.idle":"2022-07-09T19:18:16.940743Z","shell.execute_reply.started":"2022-07-09T19:18:14.169634Z","shell.execute_reply":"2022-07-09T19:18:16.939734Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_padded.shape,test_padded.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:18:16.942881Z","iopub.execute_input":"2022-07-09T19:18:16.943253Z","iopub.status.idle":"2022-07-09T19:18:16.950928Z","shell.execute_reply.started":"2022-07-09T19:18:16.943215Z","shell.execute_reply":"2022-07-09T19:18:16.949820Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#after getting the padded_docs you have to use predefined glove vectors to get 300 dim representation for each word\n# we will be storing this data in form of an embedding matrix and will use it while defining our model\n# Please go through following blog's 'Example of Using Pre-Trained GloVe Embedding' section to understand how to create embedding matrix\n# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('../input/donor-choose-preprocessed/glove_vectors', 'rb') as f:\n    glov = pickle.load(f)\n    g_words =  set(glov.keys())","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:18:32.881269Z","iopub.execute_input":"2022-07-09T19:18:32.882100Z","iopub.status.idle":"2022-07-09T19:18:35.818782Z","shell.execute_reply.started":"2022-07-09T19:18:32.882062Z","shell.execute_reply":"2022-07-09T19:18:35.817800Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# create a weight matrix for words in training docs\nvocab_size = len(word_index)+1\nembedding_matrix = np.zeros((vocab_size, 300))\nfor word, i in tokenizer.word_index.items():\n\tif word in g_words:\n\t\tembedding_vector = glov[word]\n\t\tembedding_matrix[i] = embedding_vector\nembedding_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:18:38.878596Z","iopub.execute_input":"2022-07-09T19:18:38.879262Z","iopub.status.idle":"2022-07-09T19:18:39.046286Z","shell.execute_reply.started":"2022-07-09T19:18:38.879225Z","shell.execute_reply":"2022-07-09T19:18:39.045188Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"vocab_size","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:18:43.903939Z","iopub.execute_input":"2022-07-09T19:18:43.904266Z","iopub.status.idle":"2022-07-09T19:18:43.910897Z","shell.execute_reply.started":"2022-07-09T19:18:43.904239Z","shell.execute_reply":"2022-07-09T19:18:43.909791Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Categorical feature Vectorization","metadata":{}},{"cell_type":"code","source":"# for model 1 and model 2, we have to assign a unique number to each feature in a particular categorical column.\n# you can either use tokenizer,label encoder or ordinal encoder to perform the task\n# label encoder gives an error for 'unseen values' (values present in test but not in train)\n# handle unseen values with label encoder - https://stackoverflow.com/a/56876351\n# ordinal encoder also gives error with unseen values but you can use modify handle_unknown parameter\n# documentation of ordianl encoder https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html\n# after categorical feature vectorization you will have column_train_data and column_test_data.\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder()\nencoder.fit(X_tr['school_state'].values.reshape(-1,1))\nX_tr_ss = encoder.transform(X_tr['school_state'].values.reshape(-1,1))\nX_te_ss = encoder.transform(X_te['school_state'].values.reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:18:48.812183Z","iopub.execute_input":"2022-07-09T19:18:48.812535Z","iopub.status.idle":"2022-07-09T19:18:48.856017Z","shell.execute_reply.started":"2022-07-09T19:18:48.812502Z","shell.execute_reply":"2022-07-09T19:18:48.855102Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"encoder.fit(X_tr['teacher_prefix'].values.reshape(-1,1))\nX_tr_tp = encoder.transform(X_tr['teacher_prefix'].values.reshape(-1,1))\nX_te_tp = encoder.transform(X_te['teacher_prefix'].values.reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:18:49.297137Z","iopub.execute_input":"2022-07-09T19:18:49.297478Z","iopub.status.idle":"2022-07-09T19:18:49.339255Z","shell.execute_reply.started":"2022-07-09T19:18:49.297448Z","shell.execute_reply":"2022-07-09T19:18:49.338196Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"encoder.fit(X['clean_categories'].values.reshape(-1,1))\nX_tr_cc = encoder.transform(X_tr['clean_categories'].values.reshape(-1,1))\nX_te_cc = encoder.transform(X_te['clean_categories'].values.reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:18:52.643437Z","iopub.execute_input":"2022-07-09T19:18:52.644075Z","iopub.status.idle":"2022-07-09T19:18:52.688138Z","shell.execute_reply.started":"2022-07-09T19:18:52.644041Z","shell.execute_reply":"2022-07-09T19:18:52.687298Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"encoder.fit(X['clean_subcategories'].values.reshape(-1,1))\nX_tr_csc = encoder.transform(X_tr['clean_subcategories'].values.reshape(-1,1))\nX_te_csc = encoder.transform(X_te['clean_subcategories'].values.reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:18:53.128413Z","iopub.execute_input":"2022-07-09T19:18:53.129525Z","iopub.status.idle":"2022-07-09T19:18:53.174175Z","shell.execute_reply.started":"2022-07-09T19:18:53.129478Z","shell.execute_reply":"2022-07-09T19:18:53.173281Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"encoder.fit(X['project_grade_category'].values.reshape(-1,1))\nX_tr_pgc = encoder.transform(X_tr['project_grade_category'].values.reshape(-1,1))\nX_te_pgc = encoder.transform(X_te['project_grade_category'].values.reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:19:01.312864Z","iopub.execute_input":"2022-07-09T19:19:01.313293Z","iopub.status.idle":"2022-07-09T19:19:01.377907Z","shell.execute_reply.started":"2022-07-09T19:19:01.313256Z","shell.execute_reply":"2022-07-09T19:19:01.376939Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"print('Encoded train school_state shape',X_tr_ss.shape)\nprint('Encoded train teacher_prefix shape',X_tr_tp.shape)\nprint('Encoded train clean_categories shape',X_tr_cc.shape)\nprint('Encoded train clean_subcategories shape',X_tr_csc.shape)\nprint('Encoded train project_grade_category shape',X_tr_pgc.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:19:01.892242Z","iopub.execute_input":"2022-07-09T19:19:01.892575Z","iopub.status.idle":"2022-07-09T19:19:01.902165Z","shell.execute_reply.started":"2022-07-09T19:19:01.892545Z","shell.execute_reply":"2022-07-09T19:19:01.901054Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"print('Encoded test school_state shape',X_te_ss.shape)\nprint('Encoded test teacher_prefix shape',X_te_tp.shape)\nprint('Encoded test clean_categories shape',X_te_cc.shape)\nprint('Encoded test clean_subcategories shape',X_te_csc.shape)\nprint('Encoded test project_grade_category shape',X_te_pgc.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:19:06.211887Z","iopub.execute_input":"2022-07-09T19:19:06.212434Z","iopub.status.idle":"2022-07-09T19:19:06.218811Z","shell.execute_reply.started":"2022-07-09T19:19:06.212390Z","shell.execute_reply":"2022-07-09T19:19:06.217826Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"catagorical_data_train = np.hstack((X_tr_ss,X_tr_tp,X_tr_cc,X_tr_csc,X_tr_pgc))\ncatagorical_data_test = np.hstack((X_te_ss,X_te_tp,X_te_cc,X_te_csc,X_te_pgc))\ncatagorical_data_train.shape,catagorical_data_test.shape\n","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:19:07.467799Z","iopub.execute_input":"2022-07-09T19:19:07.468429Z","iopub.status.idle":"2022-07-09T19:19:07.480457Z","shell.execute_reply.started":"2022-07-09T19:19:07.468395Z","shell.execute_reply":"2022-07-09T19:19:07.479196Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Numerical feature Vectorization","metadata":{}},{"cell_type":"code","source":"# you have to standardise the numerical columns\n# stack both the numerical features\n#after numerical feature vectorization you will have numerical_data_train and numerical_data_test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"standardizer = StandardScaler()\nstandardizer.fit(X_tr['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\nX_tr_np = standardizer.transform(X_tr['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\nX_te_np = standardizer.transform(X_te['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:19:16.263631Z","iopub.execute_input":"2022-07-09T19:19:16.263977Z","iopub.status.idle":"2022-07-09T19:19:16.273271Z","shell.execute_reply.started":"2022-07-09T19:19:16.263946Z","shell.execute_reply":"2022-07-09T19:19:16.272180Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"standardizer.fit(X_tr['price'].values.reshape(-1,1))\nX_tr_price = standardizer.transform(X_tr['price'].values.reshape(-1,1))\nX_te_price = standardizer.transform(X_te['price'].values.reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:19:17.283297Z","iopub.execute_input":"2022-07-09T19:19:17.284013Z","iopub.status.idle":"2022-07-09T19:19:17.292303Z","shell.execute_reply.started":"2022-07-09T19:19:17.283981Z","shell.execute_reply":"2022-07-09T19:19:17.291214Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print('Encoded train teacher_number_of_previously_posted_projects shape',X_tr_np.shape)\nprint('Encoded train price shape',X_tr_price.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:19:21.611272Z","iopub.execute_input":"2022-07-09T19:19:21.612200Z","iopub.status.idle":"2022-07-09T19:19:21.618062Z","shell.execute_reply.started":"2022-07-09T19:19:21.612151Z","shell.execute_reply":"2022-07-09T19:19:21.616862Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"print('Encoded test teacher_number_of_previously_posted_projects shape',X_te_np.shape)\nprint('Encoded test price shape',X_te_price.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:19:23.004469Z","iopub.execute_input":"2022-07-09T19:19:23.005416Z","iopub.status.idle":"2022-07-09T19:19:23.011813Z","shell.execute_reply.started":"2022-07-09T19:19:23.005365Z","shell.execute_reply":"2022-07-09T19:19:23.010683Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"numerical_data_train = np.hstack((X_tr_np,X_tr_price))\nnumerical_data_test = np.hstack((X_te_np,X_te_price))\n\nnumerical_data_train.shape,numerical_data_test.shape\n","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:19:27.454974Z","iopub.execute_input":"2022-07-09T19:19:27.455547Z","iopub.status.idle":"2022-07-09T19:19:27.464119Z","shell.execute_reply.started":"2022-07-09T19:19:27.455511Z","shell.execute_reply":"2022-07-09T19:19:27.463190Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"X_tr_fin1 = [train_padded,X_tr_ss,X_tr_tp,X_tr_cc,X_tr_csc,X_tr_pgc,numerical_data_train]\nX_te_fin1 = [test_padded,X_te_ss,X_te_tp,X_te_cc,X_te_csc,X_te_pgc,numerical_data_test]","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:19:29.630725Z","iopub.execute_input":"2022-07-09T19:19:29.631372Z","iopub.status.idle":"2022-07-09T19:19:29.636416Z","shell.execute_reply.started":"2022-07-09T19:19:29.631336Z","shell.execute_reply":"2022-07-09T19:19:29.635294Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# https://www.geeksforgeeks.org/python-keras-keras-utils-to_categorical/\nfrom keras.utils import np_utils\ny_tr_fin = np_utils.to_categorical(y_tr, 2) \ny_te_cv = np_utils.to_categorical(y_te, 2)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:19:33.316002Z","iopub.execute_input":"2022-07-09T19:19:33.316353Z","iopub.status.idle":"2022-07-09T19:19:33.324720Z","shell.execute_reply.started":"2022-07-09T19:19:33.316322Z","shell.execute_reply":"2022-07-09T19:19:33.323424Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## 1.4 Defining the model","metadata":{}},{"cell_type":"markdown","source":"<img src='https://i.imgur.com/w395Yk9.png'>","metadata":{}},{"cell_type":"code","source":"# as of now we have vectorized all our features now we will define our model.\n# as it is clear from above image that the given model has multiple input layers and hence we have to use functional API\n# Please go through - https://keras.io/guides/functional_api/\n# it is a good programming practise to define your complete model i.e all inputs , intermediate and output layers at one place.\n# while defining your model make sure that you use variable names while defining any length,dimension or size.\n#for ex.- you should write the code as 'input_text = Input(shape=(pad_length,))' and not as 'input_text = Input(shape=(300,))'\n# the embedding layer for text data should be non trainable\n# the embedding layer for categorical data should be trainable\n# https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n# https://towardsdatascience.com/deep-embeddings-for-categorical-variables-cat2vec-b05c8ab63ac0\n#print model.summary() after you have defined the model\n#plot the model using utils.plot_model module and make sure that it is similar to the above image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"uniq_ss = X_tr['school_state'].nunique()\nuniq_tp = X_tr['clean_categories'].nunique()\nuniq_cc = X_tr['clean_categories'].nunique()\nuniq_csc = X_tr['clean_subcategories'].nunique()\nuniq_pgc = X_tr['project_grade_category'].nunique()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:19:38.130120Z","iopub.execute_input":"2022-07-09T19:19:38.130468Z","iopub.status.idle":"2022-07-09T19:19:38.172441Z","shell.execute_reply.started":"2022-07-09T19:19:38.130436Z","shell.execute_reply":"2022-07-09T19:19:38.171572Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"embd_size_ss = int(min(np.ceil((X_tr['school_state'].nunique())/2), 50 ))\nembd_size_tp = int(min(np.ceil((X_tr['clean_categories'].nunique())/2), 50 ))\nembd_size_cc = int(min(np.ceil((X_tr['clean_categories'].nunique())/2), 50 ))\nembd_size_csc = int(min(np.ceil((X_tr['clean_subcategories'].nunique())/2), 50 ))\nembd_size_pgc = int(min(np.ceil((X_tr['project_grade_category'].nunique())/2), 50 ))","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:19:41.750103Z","iopub.execute_input":"2022-07-09T19:19:41.750438Z","iopub.status.idle":"2022-07-09T19:19:41.790794Z","shell.execute_reply.started":"2022-07-09T19:19:41.750407Z","shell.execute_reply":"2022-07-09T19:19:41.789895Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:21:28.935882Z","iopub.execute_input":"2022-07-09T19:21:28.936772Z","iopub.status.idle":"2022-07-09T19:21:28.941864Z","shell.execute_reply.started":"2022-07-09T19:21:28.936738Z","shell.execute_reply":"2022-07-09T19:21:28.940608Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(filepath=\"LSTMmodel_1.h5\", monitor=\"val_auroc\", mode=\"max\", save_best_only=True, verbose=2)\nearlystop = EarlyStopping(monitor = 'val_auroc', patience = 3, mode=\"max\",min_delta = 0, verbose = True)\nlogdir = 'logs1'\ntensorboard = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:21:31.551076Z","iopub.execute_input":"2022-07-09T19:21:31.551639Z","iopub.status.idle":"2022-07-09T19:21:31.669509Z","shell.execute_reply.started":"2022-07-09T19:21:31.551587Z","shell.execute_reply":"2022-07-09T19:21:31.668476Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"from keras import regularizers\n\n\nip_seq_text_data = Input(shape=(pad_len,), name='essay')\nembd_lyr_txt = Embedding(input_dim=vocab_size, output_dim=300, input_length=300 , weights=[embedding_matrix])(ip_seq_text_data)\nlstm_lyr_txt = LSTM(128,recurrent_dropout=0.25,kernel_regularizer=regularizers.l2(0.001), return_sequences=True)(embd_lyr_txt)\nflatten_text = Flatten()(lstm_lyr_txt)\n\n#catagorical features\nschool_state = Input(shape=(X_tr_ss.shape[1],), name='school_state')\nembd_ss = Embedding(input_dim=uniq_ss, output_dim = embd_size_ss, input_length=1)(school_state)\nflatten_school_state = Flatten()(embd_ss)\n\nproject_grade_cat = Input(shape=(X_tr_tp.shape[1],), name='project_grade_category')\nembd_pgc = Embedding(input_dim=uniq_pgc,output_dim=embd_size_pgc,input_length=1)(project_grade_cat)\nflatten_project_grade_cat = Flatten()(embd_pgc)\n\nclean_categories = Input(shape=(X_tr_cc.shape[1],), name='clean_categories')\nembd_cc = Embedding(input_dim=uniq_cc,output_dim=embd_size_cc,input_length=1)(clean_categories)\nflatten_clean_categories = Flatten()(embd_cc)\n\nclean_subcategories = Input(shape=(X_tr_csc.shape[1],), name='clean_subcategories')\nembd_csc = Embedding(input_dim=uniq_csc, output_dim=embd_size_csc, input_length=1)(clean_subcategories)\nflatten_clean_subcategories = Flatten()(embd_csc)\n\nteacher_prefix = Input(shape=(X_te_tp.shape[1],), name='teacher_prefix')\nembd_tp = Embedding( input_dim=uniq_tp,output_dim=embd_size_tp,input_length=1)(teacher_prefix)\nflatten_teacher_prefix = Flatten()(embd_tp)\n\n\n## Numerical features\nnumerical_features = Input(shape=(2,) , name=\"numerical_features\")\nnum_dense_layer = Dense(64, activation='relu', kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(0.001))(numerical_features)\n\n## concatenating all the parallel layers descibed above\nconcat_parallel_layers = concatenate([flatten_text , flatten_school_state , flatten_project_grade_cat,\nflatten_clean_categories , flatten_clean_subcategories , flatten_teacher_prefix , num_dense_layer])\n\n\n# Dense layer after concatinating\ndense_1 = Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(0.001))(concat_parallel_layers)\ndropout_lyr_1_1 = Dropout(0.5)(dense_1)\ndense_con2 = Dense(256,activation=\"relu\",kernel_initializer=\"he_normal\" ,kernel_regularizer=regularizers.l2(0.001))(dropout_lyr_1_1)\nBatchNormalization_1 = BatchNormalization()(dense_con2)\ndropout_lyr_1_2 = Dropout(0.3)(BatchNormalization_1)\ndense_3 = Dense(96,activation=\"relu\", kernel_initializer=\"he_normal\" ,kernel_regularizer=regularizers.l2(0.001))(dropout_lyr_1_2)\n\n\n## output\noutput = Dense(2, activation='softmax', name='output')(dense_3)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:21:41.853667Z","iopub.execute_input":"2022-07-09T19:21:41.854015Z","iopub.status.idle":"2022-07-09T19:21:45.256387Z","shell.execute_reply.started":"2022-07-09T19:21:41.853986Z","shell.execute_reply":"2022-07-09T19:21:45.255441Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"## adding input and output to the model\nmodel_1 = Model(inputs=[ip_seq_text_data, school_state, project_grade_cat, clean_categories, clean_subcategories, teacher_prefix, numerical_features], outputs=output)","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:21:53.515065Z","iopub.execute_input":"2022-07-09T19:21:53.515405Z","iopub.status.idle":"2022-07-09T19:21:53.526367Z","shell.execute_reply.started":"2022-07-09T19:21:53.515370Z","shell.execute_reply":"2022-07-09T19:21:53.525130Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"model_1.summary()","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:21:54.261582Z","iopub.execute_input":"2022-07-09T19:21:54.262497Z","iopub.status.idle":"2022-07-09T19:21:54.272095Z","shell.execute_reply.started":"2022-07-09T19:21:54.262448Z","shell.execute_reply":"2022-07-09T19:21:54.270919Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"## 1.5 Compiling and fititng your model","metadata":{}},{"cell_type":"code","source":"#define custom auc as metric , do not use tf.keras.metrics\n# https://stackoverflow.com/a/46844409 - custom AUC reference 1\n# https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/80807  - custom AUC reference 2\n# compile and fit your model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras/46844409#46844409\nfrom sklearn.metrics import roc_auc_score\ndef auc(y_train, y_pred):\n    if len(np.unique(y_train[:,1])) == 1:\n        return 0.5\n    else:\n        return roc_auc_score(y_train, y_pred)\n\ndef auroc(y_train, y_pred):\n    return tf.py_function(auc, (y_train, y_pred), tf.double)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:22:04.387814Z","iopub.execute_input":"2022-07-09T19:22:04.388177Z","iopub.status.idle":"2022-07-09T19:22:04.394447Z","shell.execute_reply.started":"2022-07-09T19:22:04.388147Z","shell.execute_reply":"2022-07-09T19:22:04.393467Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"model_1.compile(loss='categorical_crossentropy',\n                optimizer='Adam',\n                metrics=[auroc])","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:22:08.120402Z","iopub.execute_input":"2022-07-09T19:22:08.120775Z","iopub.status.idle":"2022-07-09T19:22:08.137787Z","shell.execute_reply.started":"2022-07-09T19:22:08.120743Z","shell.execute_reply":"2022-07-09T19:22:08.136761Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# !rm -rf ./logs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"M1 = model_1.fit(\n    x = X_tr_fin1,\n    y = y_tr_fin,\n    batch_size=64,\n    epochs=4,\n    verbose=2,\n    callbacks = [checkpoint,earlystop,tensorboard],\n    # validation_split=0.0,\n    validation_data=(X_te_fin1, y_te_cv),  \n    )","metadata":{"execution":{"iopub.status.busy":"2022-07-09T19:22:26.923801Z","iopub.execute_input":"2022-07-09T19:22:26.924156Z","iopub.status.idle":"2022-07-09T20:51:51.134413Z","shell.execute_reply.started":"2022-07-09T19:22:26.924126Z","shell.execute_reply":"2022-07-09T20:51:51.132750Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"model_1.save_weights('./m1')","metadata":{"execution":{"iopub.status.busy":"2022-07-09T20:51:51.137733Z","iopub.status.idle":"2022-07-09T20:51:51.138417Z","shell.execute_reply.started":"2022-07-09T20:51:51.138157Z","shell.execute_reply":"2022-07-09T20:51:51.138181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard\n","metadata":{"execution":{"iopub.status.busy":"2022-07-09T20:57:22.363946Z","iopub.execute_input":"2022-07-09T20:57:22.364333Z","iopub.status.idle":"2022-07-09T20:57:22.370872Z","shell.execute_reply.started":"2022-07-09T20:57:22.364283Z","shell.execute_reply":"2022-07-09T20:57:22.369765Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"!kill 3452","metadata":{"execution":{"iopub.status.busy":"2022-07-09T20:57:13.204931Z","iopub.execute_input":"2022-07-09T20:57:13.205847Z","iopub.status.idle":"2022-07-09T20:57:13.930724Z","shell.execute_reply.started":"2022-07-09T20:57:13.205810Z","shell.execute_reply":"2022-07-09T20:57:13.929378Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"%tensorboard --logdir logs1","metadata":{"execution":{"iopub.status.busy":"2022-07-09T20:57:28.873147Z","iopub.execute_input":"2022-07-09T20:57:28.873507Z","iopub.status.idle":"2022-07-09T20:57:32.948469Z","shell.execute_reply.started":"2022-07-09T20:57:28.873478Z","shell.execute_reply":"2022-07-09T20:57:32.947337Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# !rm -rf ./logs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color='red'> Model-2 </font>","metadata":{}},{"cell_type":"markdown","source":"Use the same model as above but for 'input_seq_total_text_data' give only some words in the sentance not all the words. Filter the words as below. ","metadata":{}},{"cell_type":"markdown","source":"<pre>\n1. Fit TF-IDF vectorizer on the Train data <br>\n2. Get the idf value for each word we have in the train data. Please go through <a  href='https://stackoverflow.com/questions/23792781/tf-idf-feature-weights-using-sklearn-feature-extraction-text-tfidfvectorizer'>this</a><br>\n\n3. Do some analysis on the Idf values and based on those values choose the low and high threshold value. Because very \nfrequent words and very very rare words don't give much information.\nHint - A preferable IDF range is 2-11 for model 2. <br>\n4.Remove the low idf value and high idf value words from the train and test data. You can go through each of the\nsentence of train and test data and include only those features(words) which are present in the defined IDF range.\n5. Perform tokenization on the modified text data same as you have done for previous model.\n6. Create embedding matrix for model 2 and then use the rest of the features similar to previous model.\n7. Define the model, compile and fit the model.\n</pre>","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nvectorizer.fit(X_tr['essay'])\nessay_vect = vectorizer.transform(X_tr['essay'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idf = {'word': vectorizer.get_feature_names() , 'idf_score': vectorizer.idf_}\nidf_scr = idf['idf_score']\nsorted_idf = idf_scr.sort()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(idf_scr)\nplt.grid()\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df= pd.DataFrame(idf)\nprint(df.shape)\ndf.head(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(df[df['idf_score'] < 2.0].index, inplace = True)\ndf.drop(df[df['idf_score']> 11.0].index, inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.head(2))\nprint()\nprint(df.tail(2))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = np.array(df['word'])\n\n# Tokenize our training data\ntokenizer2 = Tokenizer()\ntokenizer2.fit_on_texts(train_data)\n\n# Get our training data word index\nword_index = tokenizer2.word_index\n\n# Encode training data sentences into sequences\ntrain_sequences = tokenizer2.texts_to_sequences(X_tr['essay'])\n\n# Get max training sequence length\nmaxlen = 300\n# Pad the training sequences\ntrain_padded2= pad_sequences(train_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\npad_len2 = train_padded.shape[1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = X_te['essay']\ntest_sequences = tokenizer2.texts_to_sequences(X_te['essay'])\ntest_padded2 = pad_sequences(test_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_padded2.shape,test_padded2.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('glove_vectors', 'rb') as f:\n    glov = pickle.load(f)\n    g_words =  set(glov.keys())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a weight matrix for words in training docs\nvocab_size2 = len(word_index)+1\nembedding_matrix2 = np.zeros((vocab_size2, 300))\nfor word, i in tokenizer2.word_index.items():\n\tif word in g_words:\n\t\tembedding_vector2 = glov[word]\n\t\tembedding_matrix2[i] = embedding_vector2\nembedding_matrix2.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tr_fin2 = [train_padded2,X_tr_ss,X_tr_tp,X_tr_cc,X_tr_csc,X_tr_pgc,numerical_data_train]\nX_te_fin2 = [test_padded2,X_te_ss,X_te_tp,X_te_cc,X_te_csc,X_te_pgc,numerical_data_test]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(filepath=\"LSTMmodel_2.h5\", monitor=\"val_auroc\", mode=\"max\", save_best_only=True, verbose=2)\nearlystop = EarlyStopping(monitor = 'val_auroc', patience = 3, mode=\"max\",min_delta = 0, verbose = 2)\nlogdir = 'logs2'\ntensorboard = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import regularizers\n\n\nip_seq_text_data = Input(shape=(pad_len2,), name='essay2')\nembd_lyr_txt = Embedding(input_dim=vocab_size2, output_dim=300, input_length=300 , weights=[embedding_matrix2])(ip_seq_text_data)\nlstm_lyr_txt = LSTM(128,recurrent_dropout=0.25,kernel_regularizer=regularizers.l2(0.001), return_sequences=True)(embd_lyr_txt)\nflatten_text = Flatten()(lstm_lyr_txt)\n\n#catagorical features\nschool_state = Input(shape=(X_tr_ss.shape[1],), name='school_state')\nembd_ss = Embedding(input_dim=uniq_ss, output_dim = embd_size_ss, input_length=1)(school_state)\nflatten_school_state = Flatten()(embd_ss)\n\nproject_grade_cat = Input(shape=(X_tr_tp.shape[1],), name='project_grade_category')\nembd_pgc = Embedding(input_dim=uniq_pgc,output_dim=embd_size_pgc,input_length=1)(project_grade_cat)\nflatten_project_grade_cat = Flatten()(embd_pgc)\n\nclean_categories = Input(shape=(X_tr_cc.shape[1],), name='clean_categories')\nembd_cc = Embedding(input_dim=uniq_cc,output_dim=embd_size_cc,input_length=1)(clean_categories)\nflatten_clean_categories = Flatten()(embd_cc)\n\nclean_subcategories = Input(shape=(X_tr_csc.shape[1],), name='clean_subcategories')\nembd_csc = Embedding(input_dim=uniq_csc, output_dim=embd_size_csc, input_length=1)(clean_subcategories)\nflatten_clean_subcategories = Flatten()(embd_csc)\n\nteacher_prefix = Input(shape=(X_te_tp.shape[1],), name='teacher_prefix')\nembd_tp = Embedding( input_dim=uniq_tp,output_dim=embd_size_tp,input_length=1)(teacher_prefix)\nflatten_teacher_prefix = Flatten()(embd_tp)\n\n\n## Numerical features\nnumerical_features = Input(shape=(2,) , name=\"numerical_features\")\nnum_dense_layer = Dense(64, activation='relu', kernel_initializer='he_normal',kernel_regularizer=regularizers.l2(0.001))(numerical_features)\n\n## concatenating all the parallel layers descibed above\nconcat_parallel_layers = concatenate([flatten_text , flatten_school_state , flatten_project_grade_cat,\nflatten_clean_categories , flatten_clean_subcategories , flatten_teacher_prefix , num_dense_layer])\n\n\n# Dense layer after concatinating\ndense_1 = Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.l2(0.001))(concat_parallel_layers)\ndropout_lyr_2_1 = Dropout(0.5)(dense_1)\ndense_con2 = Dense(256,activation=\"relu\",kernel_initializer=\"he_normal\" ,kernel_regularizer=regularizers.l2(0.001))(dropout_lyr_2_1)\nBatchNormalization_1 = BatchNormalization()(dense_con2)\ndropout_lyr_2_2 = Dropout(0.3)(BatchNormalization_1)\ndense_3 = Dense(96,activation=\"relu\", kernel_initializer=\"he_normal\" ,kernel_regularizer=regularizers.l2(0.001))(dropout_lyr_2_2)\n\n\n## output\noutput = Dense(2, activation='softmax', name='output')(dense_3)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## adding input and output to the model\nmodel_2 = Model(inputs=[ip_seq_text_data, school_state, project_grade_cat, clean_categories, clean_subcategories, teacher_prefix, numerical_features], outputs=output)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2.compile(loss='categorical_crossentropy',\n                optimizer='Adam',\n                metrics=[auroc])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"M2 = model_2.fit(\n    x = X_tr_fin2,\n    y = y_tr_fin,\n    batch_size=64,\n    epochs=4,\n    verbose=2,\n    callbacks = [checkpoint,earlystop,tensorboard],\n    # validation_split=0.0,\n    validation_data=(X_te_fin2, y_te_cv),  \n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2.save('D:\\Applied_AI\\Ass_26_LSTM\\weights\\m2')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%tensorboard --logdir logs2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf ./logs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color='red'> Model-3 </font>","metadata":{}},{"cell_type":"markdown","source":"<img src='https://i.imgur.com/fkQ8nGo.png'>\nref: https://i.imgur.com/fkQ8nGo.png","metadata":{}},{"cell_type":"code","source":"#in this model you can use the text vectorized data from model1 \n#for other than text data consider the following steps\n# you have to perform one hot encoding of categorical features. You can use onehotencoder() or countvectorizer() for the same.\n# Stack up standardised numerical features and all the one hot encoded categorical features\n#the input to conv1d layer is 3d, you can convert your 2d data to 3d using np.newaxis\n# Note - deep learning models won't work with sparse features, you have to convert them to dense features before fitting in the model.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_padded3 = train_padded\ntest_padded3 = test_padded\ntrain_padded3.shape,test_padded3.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nencoder.fit(X_tr['school_state'].values.reshape(-1,1))\nX_tr_ss_ohe = encoder.transform(X_tr['school_state'].values.reshape(-1,1))\nX_te_ss_ohe = encoder.transform(X_te['school_state'].values.reshape(-1,1))\nX_tr_ss_ohe.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder.fit(X_tr['teacher_prefix'].values.reshape(-1,1))\nX_tr_tp_ohe = encoder.transform(X_tr['teacher_prefix'].values.reshape(-1,1))\nX_te_tp_ohe = encoder.transform(X_te['teacher_prefix'].values.reshape(-1,1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder.fit(X['clean_categories'].values.reshape(-1,1))\nX_tr_cc_ohe = encoder.transform(X_tr['clean_categories'].values.reshape(-1,1))\nX_te_cc_ohe = encoder.transform(X_te['clean_categories'].values.reshape(-1,1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder.fit(X['clean_subcategories'].values.reshape(-1,1))\nX_tr_csc_ohe = encoder.transform(X_tr['clean_subcategories'].values.reshape(-1,1))\nX_te_csc_ohe = encoder.transform(X_te['clean_subcategories'].values.reshape(-1,1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder.fit(X['project_grade_category'].values.reshape(-1,1))\nX_tr_pgc_ohe = encoder.transform(X_tr['project_grade_category'].values.reshape(-1,1))\nX_te_pgc_ohe = encoder.transform(X_te['project_grade_category'].values.reshape(-1,1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Encoded train school_state shape',X_tr_ss_ohe.shape)\nprint('Encoded train teacher_prefix shape',X_tr_tp_ohe.shape)\nprint('Encoded train clean_categories shape',X_tr_cc_ohe.shape)\nprint('Encoded train clean_subcategories shape',X_tr_csc_ohe.shape)\nprint('Encoded train project_grade_category shape',X_tr_pgc_ohe.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Encoded test school_state shape',X_te_ss_ohe.shape)\nprint('Encoded test teacher_prefix shape',X_te_tp_ohe.shape)\nprint('Encoded test clean_categories shape',X_te_cc_ohe.shape)\nprint('Encoded test clean_subcategories shape',X_te_csc_ohe.shape)\nprint('Encoded test project_grade_category shape',X_te_pgc_ohe.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tr_np3 = X_tr_np\nX_te_np3 = X_te_np\nX_tr_np3.shape,X_te_np3.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tr_price3 = X_tr_price\nX_te_price3 = X_te_price\nX_tr_price3.shape,X_te_price3.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tr_ss_ohe.shape, X_tr_tp_ohe.shape , X_tr_cc_ohe.shape, X_tr_csc_ohe.shape , X_tr_pgc_ohe.shape , X_tr_np3.shape , X_tr_price3.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.sparse import hstack\nX_tr_cat_num = hstack((X_tr_ss_ohe,X_tr_tp_ohe,X_tr_cc_ohe,X_tr_csc_ohe,X_tr_pgc_ohe,X_tr_np3,X_tr_price3)).todense()\nX_te_cat_num = hstack((X_te_ss_ohe,X_te_tp_ohe,X_te_cc_ohe,X_te_csc_ohe,X_te_pgc_ohe,X_te_np3,X_te_price3)).todense()\nX_tr_cat_num.shape,X_te_cat_num.shape,","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.geeksforgeeks.org/insert-a-new-axis-within-a-numpy-array/\nX_tr_cat_num3d = X_tr_cat_num[...,np.newaxis]\nX_te_cat_num3d = X_te_cat_num[...,np.newaxis]\nX_tr_cat_num3d.shape, X_te_cat_num3d.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tr_fin3 = [train_padded3,X_tr_cat_num3d]\nX_te_fin3 = [test_padded3,X_te_cat_num3d]\nlen(X_tr_fin3),len(X_tr_fin3[0]),len(X_tr_fin3[0][0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(filepath=\"./LSTMmodel_3.h5\", monitor=\"val_auroc\", mode=\"max\", save_best_only=True, verbose=2)\nearlystop = EarlyStopping(monitor = 'val_auroc', patience = 5, mode=\"max\",min_delta = 0, verbose = 2)\nlogdir = 'logs3'\ntensorboard = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pad_len","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ip_seq_text_data3 = Input(shape=(pad_len,), name='essay3')\nembd_lyr_txt = Embedding(input_dim=vocab_size, output_dim=300, input_length=300 , weights=[embedding_matrix])(ip_seq_text_data3)\nlstm_lyr_txt = LSTM(128,recurrent_dropout=0.25,kernel_regularizer=regularizers.l2(0.001), return_sequences=True)(embd_lyr_txt)\nflatten_text = Flatten()(lstm_lyr_txt)\n\ncat_num  = Input(shape=(514,1), name='cat and num')\nconv3_1 = Conv1D(128 ,3 ,activation='relu', kernel_initializer=he_normal(seed=None) , padding='valid')(cat_num)\nconv3_2 = Conv1D(64 ,3 ,activation='relu', kernel_initializer=he_normal(seed=None) , padding='valid')(conv3_1)\ncat_num_flatten_3 = Flatten()(conv3_2)\n\n## combining the parallel branches\nconcat_m3 = concatenate([flatten_text, cat_num_flatten_3])\n\ndense_lyr_3_1 = Dense(128, activation=\"relu\", kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2(0.001))(concat_m3)\n\ndropout_lyr_3_1 = Dropout(0.2)(dense_lyr_3_1)\n\ndense_lyr_3_2 = Dense(256,activation=\"relu\",kernel_initializer=\"he_normal\" ,kernel_regularizer=regularizers.l2(0.001))(dropout_lyr_3_1)\n\ndropout_lyr_3_2 = Dropout(0.2)(dense_lyr_3_2)\n\ndense_lyr_3_3 = Dense(128, activation=\"relu\", kernel_initializer=\"he_normal\", kernel_regularizer=regularizers.l2(0.001))(dropout_lyr_3_2)\n\noutput = Dense(2, activation='softmax', name='output')(dense_lyr_3_3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ip_seq_text_data3.shape,cat_num.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_3 = Model(inputs=[ip_seq_text_data3, cat_num],outputs=output)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_3.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_3.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=[auroc])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_3.fit(\n    x = X_tr_fin3,\n    y = y_tr_fin,\n    batch_size=128,\n    epochs =4,\n    verbose=2,\n    validation_data=(X_te_fin3, y_te_cv),\n    callbacks=[checkpoint,earlystop,tensorboard]\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_3.save('D:\\Applied_AI\\Ass_26_LSTM\\weights\\m3')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%tensorboard --logdir logs","metadata":{},"execution_count":null,"outputs":[]}]}